#B. terricola alignment to Bimp (

#Step1: Unzip files
gunzip *fastq.gz
----------------------------------------------------------------------------------------------------------------------------------
#Step2: Trimming of adapter
To trim
ref: Bombus pipeline Trimmed script
Need a beelist, made one for Bombus at /media/data1/bombus/fastq
*In Linux, CR cannot be used. Only LF Make sure CR is deleted*

To run Trimmomatic, go to scripts/Bombus_pipeline
nohup ./pipeline_trimBcola.sh /media/data1/bombus/fastq/subsetalign /media/data1/bombus/fastq/subsetalign/beelist.txt /media/data1/bombus/fastq/subsetalign/trimmed &

Note: ./pipline.trimBcola.sh Brock edited. Make sure to change the adaptors follow either TCAG or Nanuq
Change threads to number of CPU usage
-----------------------------------------------------------------------------------------------------------------------------------
#Step3: BWA step
For multiple bees, aligining by BWA,
e.g.,for f in /media/data1/afz/trimmed_fastq/*.fastq
	do bwa aln -t20 /home/amel45/AM45/am45new.fasta $f > $f.sai &
done

actual run example:
for f in /media/data1/bombus/fastq/subsetalign/trimmed/*.fastq
	do bwa aln -t2 /media/data1/bombus/BimpFullGenome.fa $f > $f.sai &
done

*note for #CPUs each sample goes through number of cpus specified. So if you have 3 fastq files & t=2, then 6 cpus will be used
----------------------------------------------------------------------------------------------------------------------------------------------
#Step 4: Making BAM files, for multiple bees using script /media/data1/bombus/heather/bmel/B_melanopygus_FASTQ/Bmel_melsubset/sampe_run.sh
Ref: /media/data1/afz/git/AlignmentCreation.sh

filename='beelist.txt'
exec 4<$filename
echo Start
while read -u4 p ; do
	pidlist_sampe=""
   #LDB23S_R1.fastq.sai
	#echo $p
	endfq="_R1.fastq"
	endfq2="_R2.fastq"
	sai=".sai"
	fq1=$p$endfq
	fq2=$p$endfq2
	r1=$fq1$sai
	r2=$fq2$sai
	bwa sampe -r "@RG\tID:$p\tPL:illumina\tLB:$p\tPU:run\tSM:$p" /media/data1/bombus/BimpFullGenome.fa $r1 $r2 $fq1 $fq2 | samtools view -Sb - | samtools sort -@ 5 - $p.1.sorted &
done

#Note: If sampe doesn't work, check for CRs in sampe_run script & change beelist path to the directory of running. And change the filename.
#There could be a merging step after this if the one bee is run in two lanes, some Apis bees were.
----------------------------------------------------------------------------------------------------------------------------------------------
#Step 5: Making a BAM file through use of stampy and splitter.

#Python script in /scripts/splitter/stampy_splitter_bwa
#Check number of cpus before you run it, 2 per bee
/scripts/splitter/stampy_splitter_bwa_alivia [CPU=2]
#You can change the divergence parameter

#First create index through stampy (not sure how, haven't done it myself)
#Bombus terrestris referencing in /media/data1/bombus
#File: BterrFullGenome.stidx but when you give the file name, don't give the extension stdix

Stampy command, run it from the folder containing bam file for one bee
./stampy_splitter_bwa out.bam in.bam [0.05] [ref.fa]

python /scripts/splitter/stampy_splitter_bwa_alivia /media/data1/bombus/trimmed/Bcola_1_out.bam /media/data1/bombus/trimmed/Bcola_1.bam 0.05 /media/data1/bombus/BterrFullGenome &

For multiple bees, use the script in /media/data1/bombus/heather/bmel/B_melanopygus_FASTQ/Bmel_melsubset/stampy_run.sh

filename='beelist.txt'
exec 4<$filename
echo Start
while read -u4 p ; do
	python /scripts/splitter/stampy_splitter_bwa_alivia $p.stampy.bam $p.bam 0.001 /media/data1/bombus/BimpFullGenome &
done

#For multiple bees, use the script in /media/data1/bombus/heather/bmel/B_melanopygus_FASTQ/Bmel_melsubset/stampy_run.sh
#*Note: change previous bam file names ".1.sorted" to ".bam"
#Divergence used between terricola & bimp is 0.001 
------------------------------------------------------------------------------------------------------------------------------------
#Step 6: Removing duplicates(ref see GATK documentation,) using picard 

For multiple bees,
#Remove dups and add read groups
filename='beelist.txt'
exec 4<$filename
echo Start
while read -u4 p ; do
	export PICARD=/usr/lib/picard-tools-1.80
	picard MarkDuplicates.jar I=$p.stampy.bam  O=$p.stampy.dp.bam METRICS_FILE=$p.Dups VALIDATION_STRINGENCY=SILENT MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000
	picard AddOrReplaceReadGroups.jar INPUT= $p.stampy.dp.bam   OUTPUT=$p.stampy.rg.dp.bam RGID=$p RGPL=illumina RGLB=$p RGPU=run RGSM=$p VALIDATION_STRINGENCY=LENIENT
	samtools index $p.stampy.rg.dp.bam
done

For multiple bees, use the script in /media/data1/bombus/fastq/subsetalign/GATK/dups_readgrps.sh
-------------------------------------------------------------------------------------------------------------------------------------------------
#Step:7 #Align indels by GATK

For multiple bees,
#GATK indel Realigner
#change number of cpus -nt(number of threads)

e.g.,
filename='beelist.txt'
exec 4<$filename
echo Start
while read -u4 p ; do
	gatk -T RealignerTargetCreator -R /media/data1/bombus/BimpFullGenome.fa -nt 5 -I $p.stampy.rg.dp.bam  -o $p.intervals
	gatk -T IndelRealigner -R /media/data1/bombus/BimpFullGenome.fa -I $p.stampy.rg.dp.bam  -targetIntervals  $p.intervals -o $p.stampy.rg.dp.gatk.bam
done

-------------------------------------------------------------------------------------------
#Step 8: Depth of coverage for all sites

nohup gatk -R /media/data1/bombus/BimpFullGenome.fa -T DepthOfCoverage -o Bmel3bees_coverage -I BmelR003.stampy.rg.dp.gatk.bam -I R001.stampy.rg.dp.gatk.bam -I R007.stampy.rg.dp.gatk.bam &
---------------------------------------------------------------------------------------------
#Variant & indel calling by GATK
#change nt: number of threads
#VCF files are in /media/data1/bombus/vcf_Bmel_to_Bimp

#SNP
gatk -R /media/data1/bombus/BimpFullGenome.fa -T UnifiedGenotyper  -I /media/data1/bombus/fastq/subsetalign/bam_Ontario8/bam.list -o BcolatoBimpOntario8.raw.vcf -stand_call_conf 60.0 -stand_emit_conf 40.0 -dcov 200 --min_base_quality_score 20 -nt 10 -glm SNP -ploidy 2 &

#indel
gatk -R /media/data1/bombus/BimpFullGenome.fa -T UnifiedGenotyper  -I /media/data1/bombus/fastq/subsetalign/bam_Ontario8/bam.list -o BcolatoBimpOntario8.raw.indel.vcf -stand_call_conf 60.0 -stand_emit_conf 40.0 -dcov 200 --min_base_quality_score 20 -nt 15 -glm INDEL -ploidy 2 &

example for Quebec
#SNP
gatk -R /media/data1/bombus/BimpFullGenome.fa -T UnifiedGenotyper  -I /media/data1/bombus/fastq/subsetalign/bam_Quebec9/bam.list -o BcolatoBimpQuebec.raw.vcf -stand_call_conf 60.0 -stand_emit_conf 40.0 -dcov 200 --min_base_quality_score 20 -nt 10 -glm SNP -ploidy 2 &

#indel
gatk -R /media/data1/bombus/BimpFullGenome.fa -T UnifiedGenotyper  -I /media/data1/bombus/fastq/subsetalign/bam_Quebec9/bam.list -o BcolatoBimpQuebec.raw.indel.vcf -stand_call_conf 60.0 -stand_emit_conf 40.0 -dcov 200 --min_base_quality_score 20 -nt 10 -glm INDEL -ploidy 2 &

--------------------------------------------------------------------------------------------------------
#Step 9: Remove sites around indels

gatk -R /media/data1/bombus/BimpFullGenome.fa -T VariantFiltration -V BcolatoBimpQuebec.raw.vcf -o BcolatoBimp.indel.vcf --mask  BcolatoBimpQuebec.raw.indel.vcf --maskExtension 10 --maskName "InDel" 

vcftools --vcf BcolatoBimp.indel.vcf --recode --remove-filtered-all --out BcolatoBimp.indel #output named as BcolatoBimp.indel.recode.vcf
----------------------------------------------------------------------------------------------------------
#Step 10: Make file depth filter (average coverage per SNP site)

vcftools --vcf BcolatoBimp.indel.recode.vcf --site-mean-depth

#Coded in a separate Rscript, but here is some of the code within:
R
depth=read.table(file="out.ldepth.mean",header=T)
1.5*IQR(depth$MEAN_DEPTH)+quantile(depth$MEAN_DEPTH,0.75) #maxdp - add to VCF command below
quantile(depth$MEAN_DEPTH,0.25)-1.5*IQR(depth$MEAN_DEPTH) #mindp
For BterricolatoBimp, 
> 1.5*IQR(depth$MEAN_DEPTH)+quantile(depth$MEAN_DEPTH,0.75)
    75%
17.7857/17.1667(Quebec)/13.82144(Ontario)
> quantile(depth$MEAN_DEPTH,0.25)-1.5*IQR(depth$MEAN_DEPTH)
   25%
5.7857/6.0555(Quebec)/4.964275(Ontario)

#basically, just trim at whatever these values are.
vcftools --vcf BcolatoBimp.indel.recode.vcf --recode --remove-filtered-all  --out BcolatoBimp.indel.dp  --min-meanDP 6 --max-meanDP 18
#Note, the maxDP filter doesn't work always, so we will filter it out later.
-------------------------------------------------------------------------------------------------------------
#Step 11: Make file (BLAST filter) Bimp.indel.dp.bl.recode.vcf

vcftools --vcf BcolatoBimp.indel.dp.recode.vcf  --exclude-positions /media/data1/bombus/MaskedSNPs/BimpAllSNPs --recode --remove-filtered-all  --out BcolatoBimp.indel.dp.bl
--------------------------------------------------------------------------------------------------------------
[Make file (diploid drone calls) Bimp.indel.dp.bl.dd.recode.vcf
	#Bimp.raw.DIPLOID.vcf contains a set of SNOs called when diploid
	#Had to remove 88 and 89
	#vcftools --vcf Bimp.raw.DIPLOID.vcf  --remove-indv "Bimp-89" --remove-indv "Bimp-88" --recode

vcftools --vcf Bimp.DIPLOID.raw.vcf --hardy 

R
hwe=read.table(file="out.hwe",header=T)
hets=apply(hwe[3],1,function(x) unlist(strsplit(x, "/"))[2])
hwe1=hwe[c(1,2)][which(hets>0),]
write.table(file="/media/data1/bombus/MaskedSNPs/DiploidBimp",hwe1,col.names=F,row.names=F,quote=F)

]
#Part within parentheses is already done. Brock outputted a file with diploid drone positions and 5 bp around it to be removed
#/media/data1/bombus/MaskedSNPs/BimpDiploidSNPs5

vcftools --vcf BcolatoBimp.indel.dp.recode.vcf  --exclude-positions /media/data1/bombus/MaskedSNPs/BimpDiploidSNPs5 --recode --remove-filtered-all --out BcolatoBimp.indel.dp.bl.dd
----------------------------------------------------------------------------------------------------------------
#Step 12: Second depth filter incase maxDP doesn't work
#Run site mean depth on last VCF created "BtertoBimp.indel.dp.bl.dd"

vcftools --vcf BcolatoBimp.indel.dp.bl.dd.recode.vcf --site-mean-depth

In R
depth=read.table(file="out.ldepth.mean",header=T)
head(depth)
depth2 <- depth[which(depth$MEAN_DEPTH > 18),] #Listing the file with Mean Depth greater than.
depth3 <- depth2[1:2]
write.table(depth3, "BcolatoBimpONtario8MaxDP.txt", row.names=FALSE, quote=FALSE, sep="\t")

#Exclude these positions in the VCF file
vcftools --vcf BcolatoBimpOntario8.indel.dp.bl.dd.recode.vcf  --exclude-positions BcolatoBimpONtario8MaxDP.txt --recode --remove-filtered-all --out BcolatoBimpOntario8.indel.dp.bl.dd.maxdp

#Additional checking
Check depth filter again and ensure anything above maxDP is removed
vcftools --vcf BcolatoBimpOntario8.indel.dp.bl.dd.maxdp.recode.vcf --site-mean-depth

depth4 <- read.table(file="out.ldepth.mean",header=T)
head(depth4)
summary(depth4$MEAN_DEPTH) 
dp_scaffold <- aggregate(depth4$MEAN_DEPTH, by=list(depth4$CHROM), mean)
colnames(dp_scaffold)[1:2] <- c("Group", "MeanDepth")
write.table(depth4, "Depth_scaffold_BcolatoBimp.txt", row.names=FALSE, quote=FALSE, sep="\t")
------------------------------------------------------------------------------------------------------------------
#So 6 files in the end
#BcolatoBimp.raw.recode.vcf
#BcolatoBimp.indel.recode.vcf
#BcolatoBimp.indel.dp.recode.vcf
#BcolatoBimp.indel.dp.bl.recode.vcf
#BcolatoBimp.indel.dp.bl.dd.recode.vcf
#BcolatoBimp.indel.dp.bl.dd.maxdp.recode.vcf
